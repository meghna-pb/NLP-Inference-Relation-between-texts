{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CentraleSupelec - Natural language processing\n",
    "# Practical session n°8\n",
    "\n",
    "### Mohammed EL Hamidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv bert-nli-env\n",
    "!source bert-nli-env/bin/activate\n",
    "!pip install torch transformers datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This part focuses on preparing the SNLI dataset for use with a neural network. It involves loading the dataset, filtering out unusable data, tokenizing the text to convert words to numerical IDs, and finally setting up PyTorch DataLoaders to facilitate batch processing during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9824/9824 [00:03<00:00, 2661.58 examples/s]\n",
      "Map: 100%|██████████| 9842/9842 [00:03<00:00, 2589.71 examples/s]\n",
      "Map: 100%|██████████| 549367/549367 [03:19<00:00, 2746.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"snli\")\n",
    "dataset = dataset.filter(lambda example: example['label'] != -1)  # Remove examples without a label\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_dataset = tokenized_datasets['train']\n",
    "val_dataset = tokenized_datasets['validation']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in this section, we initialize our neural network model. We use a pre-trained DistilBERT model from the Hugging Face Transformers library, which is fine-tuned for a sequence classification task. The model is configured to output three labels, corresponding to the possible outcomes in the SNLI dataset. We also set up an optimizer, which is responsible for updating the model's weights during training based on the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/venv/lib/python3.8/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We iterate over the dataset in batches, calculating the loss (how well the model's predictions match the actual labels) and adjusting the model's weights to minimize this loss. The training process is repeated for a fixed number of epochs. Following training, we evaluate the model's performance on a separate validation dataset to gauge its generalization ability. The model's accuracy—how often its predictions match the true labels—is reported as a measure of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.47950792495433675\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/tp nlp/bert.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://localhost:8080/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/tp%20nlp/bert.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell://localhost:8080/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/tp%20nlp/bert.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell://localhost:8080/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/tp%20nlp/bert.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://localhost:8080/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/tp%20nlp/bert.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://localhost:8080/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/tp%20nlp/bert.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8669985775248933\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_eval_accuracy = 0\n",
    "\n",
    "for batch in val_loader:\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    total_eval_accuracy += (predictions == batch['labels']).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {total_eval_accuracy / len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
