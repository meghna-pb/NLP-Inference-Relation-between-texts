{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CentraleSupelec - Natural language processing\n",
    "# Practical session nÂ°8\n",
    "### Mohammed EL Hamidi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Natural Language Inferencing (NLI): \n",
    "\n",
    "(NLI) is a classical NLP (Natural Language Processing) problem that involves taking two sentences (the premise and the hypothesis ), and deciding how they are related (if the premise *entails* the hypothesis, *contradicts* it, or *neither*).\n",
    "\n",
    "Ex: \n",
    "\n",
    "\n",
    "| Premise | Label | Hypothesis |\n",
    "| --- | --- | --- |\n",
    "| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping. |\n",
    "| An older and younger man smiling. | neutral | Two men are smiling and laughing at the cats playing on the floor. |\n",
    "| A soccer game with multiple males playing. | entailment | Some men are playing a sport. |\n",
    "\n",
    "### Stanford NLI (SNLI) corpus\n",
    "\n",
    "In this labwork, I propose to use the Stanford NLI (SNLI) corpus ( https://nlp.stanford.edu/projects/snli/ ), available in the *Datasets* library by Huggingface.\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    snli = load_dataset(\"snli\")\n",
    "    #Removing sentence pairs with no label (-1)\n",
    "    snli = snli.filter(lambda example: example['label'] != -1) \n",
    "\n",
    "## Subject\n",
    "\n",
    "You are asked to provide an operational Jupyter notebook that performs the task of NLI. For that, you need to tackle the following aspects of the problem:\n",
    "\n",
    "1. Loading and preprocessing the data\n",
    "2. Designing a PyTorch model that, given two sentences, decides how they are related (*entails*, *contradicts* or *neither*.)\n",
    "3. Training and evaluating the model using appropriate metrics\n",
    "4. (Optional) Allowing to play with the model (forward user sentences and visualize the prediction easily)\n",
    "5. (Optional) Providing visual insight about the model (i.e. visualizing the attention if your model is using attention)\n",
    "\n",
    "Although it is not mandatory, I suggest that you use a transformer model to perform the task. For that, you can use the *Transformer* library by Huggingface.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation will be based on several criteria:\n",
    "\n",
    "- Clarity and readability of the notebook. The notebook is the report of you project. Make it easy and pleasant to read.\n",
    "- Justification of implementation choices (i.e. the network, the cost funtion, the optimizer, ...)\n",
    "- Quality of the code. The various deeplearning and NLP labworks provide many example of good practices for designing experiments with neural networks. Use them as inspirational examples!\n",
    "\n",
    "## Additional recommendations\n",
    "\n",
    "- You are not seeking to publish a research paper! I'm not expecting state-of-the-art results! The idea of this labwork is to assess that you have integrated the skills necessary to handle textual data using deep neural network techniques.\n",
    "\n",
    "- This labwork will be evaluated but we are still here to help you! Don't hesitate to request our help if you are stuck.\n",
    "\n",
    "- If you intend to use BERT based models, let me give you an advice. The bert-base-* models available in *Transformers* need more than 12Go to be fine-tuned on GPU. To avoid memory issues, you can use several solutions: \n",
    "\n",
    "    - Use a lighter BERT based model such as DistilBERT, ALBERT, ...\n",
    "    - Train a classification model on top of BERT, whithout fine-tuning it (i.e. freezing BERT weights)\n",
    "\n",
    "## Huggingface documentations\n",
    "\n",
    "In case you want to use the huggingface *Datasets* and *Transformer* libraries (which I advice), here are some useful documentation pages:\n",
    "\n",
    "- Dataset quick tour\n",
    "\n",
    "    https://huggingface.co/docs/datasets/quicktour.html\n",
    "    \n",
    "- Documentation on data preprocessing for transformers\n",
    "\n",
    "    https://huggingface.co/transformers/preprocessing.html\n",
    "    \n",
    "- Transformer Quick tour (with distilbert example for classification).\n",
    "\n",
    "    https://huggingface.co/transformers/quicktour.html\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv bilstm-nli-env\n",
    "!source bilstm-nli-env/bin/activate\n",
    "!pip install torch torchvision torchinfo tqdm matplotlib scikit-learn datasets sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and processing\n",
    "\n",
    "\n",
    "#### This part focuses on preparing the SNLI dataset for use with a neural network. It involves loading the dataset, filtering out unusable data, tokenizing the text to convert words to numerical IDs, and finally setting up PyTorch DataLoaders to facilitate batch processing during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import os\n",
    "import torch.nn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_14/Bureau/NLP/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "snli = load_dataset(\"snli\")\n",
    "snli = snli.filter(lambda example: example['label'] != -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\") if use_cuda else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "def embed(text):\n",
    "    return embedder.encode(text)\n",
    "\n",
    "embedding_size=len(embed(\"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SNILDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        subset,\n",
    "        directory=\"./datasets/\",\n",
    "    ):\n",
    "        possible_subsets = [\"train\", \"validation\", \"test\"]\n",
    "        if subset not in possible_subsets:\n",
    "            raise ValueError(\n",
    "                \"Possible values for 'subset' are: {} (given {})\".format(\n",
    "                    possible_subsets, subset\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.subset = subset\n",
    "        self.test = subset == \"test\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(snli[self.subset])\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index,\n",
    "    ):\n",
    "        return torch.tensor((embed(snli[self.subset][index][\"premise\"]), embed(snli[self.subset][index][\"hypothesis\"])), dtype=torch.float32), snli[self.subset][index][\"label\"]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 1\n",
    "\n",
    "def get_dataloaders():\n",
    "\n",
    "    print(\"  - Dataset creation\")\n",
    "\n",
    "    train_dataset = SNILDataset(\n",
    "        subset=\"train\",\n",
    "    )\n",
    "    valid_dataset = SNILDataset(\n",
    "        subset=\"validation\",\n",
    "    )\n",
    "\n",
    "    print(f\"  - I loaded {len(train_dataset)} samples for training and {len(valid_dataset)} samples for validation\")\n",
    "\n",
    "    # Build the dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        #num_workers=num_workers,\n",
    "        pin_memory=use_cuda,\n",
    "    )\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        #num_workers=num_workers,\n",
    "        pin_memory=use_cuda,\n",
    "    )\n",
    "\n",
    "    num_classes = 3\n",
    "    print(f\"  - Number of classes is {num_classes}\")\n",
    "    input_size = tuple(train_dataset[0][0].shape)\n",
    "    print(f\"  - Input size is {input_size}\")\n",
    "\n",
    "    return train_loader, valid_loader, input_size, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Dataset creation\n",
      "  - I loaded 549367 samples for training and 9842 samples for validation\n",
      "  - Number of classes is 3\n",
      "  - Input size is (2, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/sdi-labworks-2023-2024_14-67261/ipykernel_62767/2442042016.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  return torch.tensor((embed(snli[self.subset][index][\"premise\"]), embed(snli[self.subset][index][\"hypothesis\"])), dtype=torch.float32), snli[self.subset][index][\"label\"]\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, input_size, num_classes = get_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why BiLSTM for NLI?\n",
    "#### Sequential Data Understanding\n",
    "##### BiLSTM networks are particularly well-suited for tasks involving sequential data, such as text. NLI involves understanding and reasoning over pairs of sentences (premise and hypothesis) to determine the relationship between them (entailment, contradiction, or neutral). BiLSTMs process sequences in both forward and backward directions, providing a comprehensive understanding of the context, which is crucial for capturing the nuances necessary for accurate inference.\n",
    "\n",
    "Long-Term Dependencies\n",
    "One of the key challenges in NLI is capturing long-term dependencies within and between sentences. BiLSTMs are designed to address this issue, making them capable of understanding complex sentence structures and the subtle meanings that influence the inference process.\n",
    "\n",
    "Feature Extraction\n",
    "The model's embedding layer, powered by Sentence Transformers, converts sentences into dense vectors, capturing semantic information effectively. This pre-processing step is critical for NLI, as the relationship between sentences often hinges on deep semantic similarities or differences that surface-level features might not capture.\n",
    "\n",
    "Why It Should Work\n",
    "Rich Contextual Representation\n",
    "The bidirectional nature of BiLSTMs allows the model to gather context from both the beginning and the end of a sentence, providing a richer representation of each sentence and its elements. This is beneficial for NLI, where the relation might depend on the context that precedes or follows a key piece of information.\n",
    "\n",
    "Adaptability to Different Text Lengths\n",
    "BiLSTMs are inherently adaptable to sequences of varying lengths, thanks to their recurrent structure. This flexibility is valuable for NLI tasks, where premises and hypotheses can significantly differ in length and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\tdef __init__(self, embedding_dim, hidden_dim, dropout_rate, out_dim, batch_size, device):\n",
    "\t\tsuper(BiLSTM, self).__init__()\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.embed_dim = embedding_dim\n",
    "\t\tself.hidden_size = hidden_dim\n",
    "\t\tself.directions = 2\n",
    "\t\tself.num_layers = 2\n",
    "\t\tself.concat = 4\n",
    "\t\tself.projection = nn.Linear(self.embed_dim, self.hidden_size)\n",
    "\t\tself.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers,\n",
    "\t\t\t\t\t\t\t\t\tbidirectional = True, batch_first = True, dropout = dropout_rate)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.dropout = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "\t\tself.lin1 = nn.Linear(self.hidden_size * self.directions * self.concat, self.hidden_size)\n",
    "\t\tself.lin2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\t\tself.lin3 = nn.Linear(self.hidden_size, out_dim)\n",
    "\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tfor lin in [self.lin1, self.lin2, self.lin3]:\n",
    "\t\t\tnn.init.xavier_uniform_(lin.weight)\n",
    "\t\t\tnn.init.zeros_(lin.bias)\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tself.lin1,\n",
    "\t\t\tself.relu,\n",
    "\t\t\tself.dropout,\n",
    "\t\t\tself.lin2,\n",
    "\t\t\tself.relu,\n",
    "\t\t\tself.dropout,\n",
    "\t\t\tself.lin3\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, batch):\n",
    "\t\tpremise_embed = batch[:, 0:1, :]\n",
    "\t\thypothesis_embed = batch[:, 1:2, :]\n",
    "\n",
    "\t\tpremise_proj = self.relu(self.projection(premise_embed))\n",
    "\t\thypothesis_proj = self.relu(self.projection(hypothesis_embed))\n",
    "\n",
    "\t\th0 = c0 = torch.tensor([]).new_zeros((self.num_layers * self.directions, batch.shape[0], self.hidden_size)).to(self.device)\n",
    "\n",
    "\t\t_, (premise_ht, _) = self.lstm(premise_proj, (h0, c0))\n",
    "\t\t_, (hypothesis_ht, _) = self.lstm(hypothesis_proj, (h0, c0))\n",
    "\t\t\n",
    "\t\tpremise = premise_ht[-2:].transpose(0, 1).contiguous().view(batch.shape[0], -1)\n",
    "\t\thypothesis = hypothesis_ht[-2:].transpose(0, 1).contiguous().view(batch.shape[0], -1)\n",
    "\n",
    "\t\tcombined = torch.cat((premise, hypothesis, torch.abs(premise - hypothesis), premise * hypothesis), 1)\n",
    "\t\treturn self.out(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, f_loss, device):\n",
    "    \"\"\"\n",
    "    Test a model over the loader\n",
    "    using the f_loss as metrics\n",
    "    Arguments :\n",
    "    model     -- A torch.nn.Module object\n",
    "    loader    -- A torch.utils.data.DataLoader\n",
    "    f_loss    -- The loss function, i.e. a loss Module\n",
    "    device    -- A torch.device\n",
    "    Returns :\n",
    "    \"\"\"\n",
    "\n",
    "    # We enter eval mode.\n",
    "    # This is important for layers such as dropout, batchnorm, ...\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Compute the forward propagation\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = f_loss(outputs, targets)\n",
    "\n",
    "        # Update the metrics\n",
    "        # We here consider the loss is batch normalized\n",
    "        total_loss += inputs.shape[0] * loss.item()\n",
    "        num_samples += inputs.shape[0]\n",
    "\n",
    "    return total_loss / num_samples\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def train(model, loader, f_loss, optimizer, device, dynamic_display=True):\n",
    "    \"\"\"\n",
    "    Train a model for one epoch, iterating over the loader\n",
    "    using the f_loss to compute the loss and the optimizer\n",
    "    to update the parameters of the model.\n",
    "    Arguments :\n",
    "    model     -- A torch.nn.Module object\n",
    "    loader    -- A torch.utils.data.DataLoader\n",
    "    f_loss    -- The loss function, i.e. a loss Module\n",
    "    optimizer -- A torch.optim.Optimzer object\n",
    "    device    -- A torch.device\n",
    "    Returns :\n",
    "    The averaged train metrics computed over a sliding window\n",
    "    \"\"\"\n",
    "\n",
    "    # We enter train mode.\n",
    "    # This is important for layers such as dropout, batchnorm, ...\n",
    "    model.train()\n",
    "    print(\"---- start training ----\")\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    for i, (inputs, targets) in (pbar := tqdm.tqdm(enumerate(loader))):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Compute the forward propagation\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = f_loss(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the metrics\n",
    "        # We here consider the loss is batch normalized\n",
    "        total_loss += inputs.shape[0] * loss.item()\n",
    "        num_samples += inputs.shape[0]\n",
    "        pbar.set_description(f\"Train loss : {total_loss/num_samples:.2f}\")\n",
    "\n",
    "    return total_loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "learning_rate=0.01\n",
    "hidden_dim=200\n",
    "dropout_rate=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(embedding_dim=embedding_size, hidden_dim=hidden_dim, dropout_rate=dropout_rate, out_dim=num_classes, batch_size=batch_size, device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "f_loss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint(object):\n",
    "    \"\"\"\n",
    "    Early stopping callback\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        savepath,\n",
    "        min_is_best: bool = True,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.savepath = savepath\n",
    "        self.best_score = None\n",
    "        if min_is_best:\n",
    "            self.is_better = self.lower_is_better\n",
    "        else:\n",
    "            self.is_better = self.higher_is_better\n",
    "\n",
    "    def lower_is_better(self, score):\n",
    "        return self.best_score is None or score < self.best_score\n",
    "\n",
    "    def higher_is_better(self, score):\n",
    "        return self.best_score is None or score > self.best_score\n",
    "\n",
    "    def update(self, score):\n",
    "        if self.is_better(score):\n",
    "            torch.save(self.model.state_dict(), self.savepath)\n",
    "            self.best_score = score\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    model, str(\"./best_model.pt\"), min_is_best=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class ModelCheckpoint(object):\n",
    "    \"\"\"\n",
    "    Early stopping callback\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        savepath,\n",
    "        min_is_best: bool = True,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.savepath = savepath\n",
    "        self.best_score = None\n",
    "        if min_is_best:\n",
    "            self.is_better = self.lower_is_better\n",
    "        else:\n",
    "            self.is_better = self.higher_is_better\n",
    "\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "\n",
    "    def lower_is_better(self, score):\n",
    "        return self.best_score is None or score < self.best_score\n",
    "\n",
    "    def higher_is_better(self, score):\n",
    "        return self.best_score is None or score > self.best_score\n",
    "\n",
    "    def update(self, score):\n",
    "        if self.is_better(score):\n",
    "            torch.save(self.model.state_dict(), self.savepath)\n",
    "            self.best_score = score\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    model, str(\"./best_model.pt\"), min_is_best=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- start training ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 0.89: : 17168it [1:18:36,  3.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train 1 epoch\n",
    "\n",
    "for e in range(1):\n",
    "\n",
    "    train_loss = train(\n",
    "        model=model,\n",
    "        loader=train_loader,\n",
    "        f_loss=f_loss,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Test\n",
    "    test_loss = test(model, valid_loader, f_loss, device)\n",
    "\n",
    "    updated = model_checkpoint.update(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SNILDataset(\n",
    "    subset=\"test\",\n",
    ")\n",
    "\n",
    "# Build the dataloaders\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    #num_workers=num_workers,\n",
    "    pin_memory=use_cuda,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 65.82%\n"
     ]
    }
   ],
   "source": [
    "def test(model, loader, device):\n",
    "    \"\"\"\n",
    "    Test a model over the loader\n",
    "    to compute accuracy.\n",
    "    Arguments :\n",
    "    model     -- A torch.nn.Module object\n",
    "    loader    -- A torch.utils.data.DataLoader\n",
    "    device    -- A torch.device\n",
    "    Returns :\n",
    "    Accuracy as a float\n",
    "    \"\"\"\n",
    "\n",
    "    # We enter eval mode.\n",
    "    model.eval()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Compute the forward propagation\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class by finding the max index in the logit dimension\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_predictions += predicted.eq(targets).sum().item()\n",
    "            num_samples += targets.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / num_samples\n",
    "    return accuracy\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"./best_model.pt\"))\n",
    "\n",
    "# Compute the accuracy on the test set\n",
    "test_accuracy = test(model, test_loader, device)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
